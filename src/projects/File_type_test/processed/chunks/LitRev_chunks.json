[
  {
    "chunk_id": "LitRev_para0",
    "chunk_text": "Add Date Title Publish Date Authors Purpose Abstract Method Result/data Conclusion and Discussion Limitations DOI 2025-01-15 00:00:00 Policies on Artificial Intelligence Chatbots Among Academic Publishers: A Cross-Sectional Audit 2024-06-20 00:00:00 Bhavsar, Daivat | Duffy, Laura | Jo, Hamin | Lokker, Cynthia | Haynes, R. Brian | Iorio, Alfonso | Marusic, Ana | Ng, Jeremy Y. Artificial intelligence chatbots, such as ChatGPT, are increasingly influencing academic research and publishing. This study audited the publicly available policies of 163 academic publishers in the STM association regarding chatbot usage by authors. Key findings include a 34.4% policy availability rate, with most mandating disclosure of chatbot use and prohibiting chatbot authorship. Four publishers imposed outright bans. This highlights a growing need for policies to adapt to evolving technologies to maintain scientific integrity. A cross-sectional audit was conducted from September to December 2023, examining publicly available policies of STM publishers through systematic web searches. Data extraction focused on policy elements, including permissions for chatbot use, authorship, and specific tasks like proofreading or image generation. Review and verification were done independently by two reviewers with oversight from a third. Of 163 publishers, 56 had policies on chatbot use, with 87.5% requiring disclosure and 94.6% rejecting chatbot authorship. A minority allowed chatbots for image generation (8.6%) and proofreading (9.2%). Notably, Karger Publishers required AI tools to be cited as primary sources, while others prohibited it. The study revealed that only a third of academic publishers had policies guiding chatbot use by authors, with significant variation in the scope and content of these policies. Future directions include revisiting policies in 12-18 months to assess their evolution and potential inclusion of editors and reviewers in such guidelines. The study is cross-sectional and limited to English-speaking publishers in the STM association. Non-English publishers and individual journal policies were excluded, and the analysis did not include adherence to policies or their enforcement. 10.1101/2024.06.19.24309148 Seeing the Random Forest Through the Decision Trees. Supporting Learning Health Systems from Histopathology with Machine Learning Models: Challenges and Opportunities 2024-01-01 00:00:00 Gonzalez, Ricardo | Saha, Ashirbani | Campbell, Clinton J.V. | Nejat, Peyman | Lokker, Cynthia | Norgan, Andrew P. What are the challenges and opportunities associated with implementing machine learning models in histopathology to support learning health systems? This paper examines the challenges and potential solutions when employing machine learning models in histopathology. It categorizes challenges into two groups: those requiring innovative technological advancements and those needing a conceptual shift. It also introduces an opportunity for integrating hidden information extracted by machine learning models with other healthcare data to bolster Learning Health Systems. This review article analyzes existing literature and case studies to explore machine learning applications in histopathology. Challenges and mitigation strategies are identified through literature synthesis, emphasizing digital pathology and big data integration. Key challenges include limited training datasets, computational constraints, and the need for model retraining in clinical settings. Opportunities lie in integrating diverse data types, leveraging multimodal machine learning, and addressing biases to improve outcomes. Despite the inherent limitations of machine learning models, they hold promise for advancing diagnostic accuracy and healthcare systems when integrated with broader datasets. The iterative improvement of these models and their ethical use could significantly impact clinical and research practices. The study highlights gaps in data diversity and potential biases in machine learning models. It also acknowledges the challenge of achieving universal generalizability and the need for extensive site-specific validation. 10.1016/j.jpi.2023.100347 2025-01-22 00:00:00 Performance of externally validated machine learning models based on histopathology images for the diagnosis, classification, prognosis, or treatment outcome prediction in female breast cancer: A systematic review 2024-11-05 00:00:00 Gonzalez, Ricardo | Nejat, Peyman | Saha, Ashirbani | Campbell, Clinton J.V. | Norgan, Andrew P. | Lokker, Cynthia Evaluate the effectiveness of machine learning models in breast cancer diagnosis and treatment outcomes. This systematic review evaluates the performance of externally validated machine learning models using histopathology images for breast cancer diagnosis, classification, prognosis, and treatment outcomes. Ten studies were included, showing accuracies and AUCs above 87% and 90%, respectively, for diagnostic and classification models. However, heterogeneity in methodologies and limited external validations hinder direct comparison and broad generalizability. Increasing dataset availability and standardization is recommended. Systematic review adhering to PRISMA 2020 guidelines Diagnostic models: Accuracies >87%, AUC >90%; Prognostic models: Hazard ratios 1.7-1.9; Variability in data sources and metrics; 2 studies high risk (small validation samples). The review underscores the need for robust external validations to generalize ML model performance. Standardizing methods and reporting, alongside dataset expansion, are critical for clinical applicability. ML models, while promising, are not yet uniformly ready for clinical deployment. Heterogeneous methodologies and datasets limit direct comparisons. Small validation datasets may overstate model performance. Limited external validation studies were included due to selection criteria. 10.1016/j.jpi.2023.100348 2025-01-26 00:00:00 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning 2025-01-22 00:00:00 Guo, Daya | Yang, Dejian | Zhang, Haowei | Song, Junxiao | Zhang, Ruoyu | Xu, Runxin | Zhu, Qihao | Ma, Shirong | Wang, Peiyi | Bi, Xiao | Zhang, Xiaokang | Yu, Xingkai | Wu, Yu | Wu, Z.F. | Gou, Zhibin | Shao, Zhihong | Li, Zhuoshu | Gao, Ziyi How can reasoning capabilities of large language models be improved through reinforcement learning without relying on supervised fine-tuning? This study introduces DeepSeek-R1-Zero and DeepSeek-R1, reasoning models trained via reinforcement learning. DeepSeek-R1-Zero, trained without supervised fine-tuning, demonstrates emergent reasoning behaviors but suffers from poor readability and language mixing. DeepSeek-R1 improves upon these limitations by employing multi-stage training and cold-start data. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To aid the research community, six smaller dense models distilled from DeepSeek-R1 are open-sourced. The methodology includes pure reinforcement learning for DeepSeek-R1-Zero and multi-stage training for DeepSeek-R1, which incorporates cold-start data, supervised fine-tuning, and rejection sampling. Benchmarks are evaluated using a combination of rule-based and language consistency rewards, with reasoning capability distilled to smaller models like Qwen and Llama. DeepSeek-R1 achieves 79.8% Pass@1 on AIME 2024, 97.3% on MATH-500, and 71.5% on GPQA Diamond. Distilled models like Qwen-32B outperform previous models, achieving 72.6% on AIME 2024 and 94.3% on MATH-500. DeepSeek-R1 demonstrates strong performance in reasoning and STEM benchmarks. DeepSeek-R1 advances reasoning capabilities through RL and cold-start training, performing comparably to OpenAI-o1-1217. Distillation demonstrates that reasoning patterns from larger models can significantly enhance smaller models. Future work will focus on improving general capabilities, handling language mixing, and enhancing software engineering tasks. Challenges include language mixing, reliance on supervised data for cold starts, sensitivity to prompt engineering, and suboptimal performance on software engineering tasks due to limited RL data in this domain. 2025-01-26 00:00:00 Deep learning to refine the identification of high-quality clinical research articles from the biomedical literature: Performance evaluation 2023-05-08 00:00:00 Lokker, Cynthia | Bagheri, Elham | Abdelkader, Wael | Parrish, Rick | Afzal, Muhammad | Navarro, Tamara | Cotoi, Chris | Germini, Federico | Linkins, Lori | Haynes, R. Brian | Chu, Lingyang | Iorio, Alfonso How can pretrained language models, like BioBERT, be fine-tuned to identify high-quality clinical research articles from biomedical literature for real-time literature surveillance programs? This study evaluates deep learning models fine-tuned with BERT variants to classify biomedical literature for high-quality clinical evidence. Using over 160,000 PubMed records labeled for methodological rigor, models such as BioBERT were trained and tested. The resulting DL-PLUS model achieved >99% recall, specificity ranging from 60%-77%, and improved the efficiency of identifying high-quality articles for clinical practice. This model significantly reduced workload in literature surveillance while maintaining methodological rigor. Variants of BERT (BERTBASE, BioBERT, BlueBERT, and PubMedBERT) were fine-tuned using a dataset of titles and abstracts of clinical articles. The dataset was divided into training (80%), validation (10%), and testing (10%). To address class imbalance, negative samples were undersampled to create balanced datasets. Hyperparameters were optimized via Bayesian sweeps, and top-performing models were ensembled to evaluate gains in specificity and recall. The BioBERT-based DL-PLUS model achieved >99% sensitivity and specificity of 60%-77%, significantly reducing manual workload by >60%. In prospective tests, the model accurately classified articles with high recall, improving the efficiency of daily literature surveillance for clinical use. The DL-PLUS model leverages state-of-the-art pretrained language models to refine literature retrieval, demonstrating substantial gains in recall and specificity. This approach enables faster dissemination of clinically relevant research while reducing appraisal workload. Future work includes addressing class imbalance and exploring oversampling for dataset enhancement. Limited performance on underrepresented categories such as prognosis and diagnosis, dependence on Boolean filters for preprocessing, and potential bias from dataset class imbalance. 10.1016/j.jbi.2023.104384 2025-01-26 00:00:00 Machine Learning Approaches to Retrieve High-Quality, Clinically Relevant Evidence From the Biomedical Literature: Systematic Review 2021-09-15 00:00:00 Abdelkader, Wael | Navarro, Tamara | Parrish, Rick | Cotoi, Chris | Germini, Federico | Iorio, Alfonso | Haynes, R. Brian | Lokker, Cynthia How can machine learning approaches be used effectively to retrieve high-quality clinical evidence from the biomedical literature? This study systematically reviews machine learning techniques applied to retrieve clinically relevant and high-quality evidence from biomedical literature. Among 3918 studies, 10 met inclusion criteria, focusing on supervised machine learning models with curated training datasets. Results showed machine learning achieving sensitivities of up to 95% and precision up to 86%. The review highlights opportunities for further advancements using active learning and unsupervised techniques. A systematic review was conducted, searching nine databases up to July 2020. Extracted data included applied machine learning models, development steps, and performance metrics. All selected studies employed supervised learning, with common preprocessing steps like stop-word removal and term weighting, and algorithms such as support vector machines (SVM) and neural networks. Machine learning classifiers achieved sensitivities ranging from 85% to 95% and precision up to 86%, significantly improving the efficiency of literature retrieval. SVMs were the most commonly used algorithms, and neural networks showed promise for further improving precision. Machine learning techniques are effective in retrieving high-quality clinical evidence, with potential for improvement through active and unsupervised learning approaches. Challenges include limited availability of high-quality datasets and inconsistent reporting standards. Constraints include reliance on supervised learning, limited training datasets, lack of standardized reporting, and challenges with dataset generalizability across medical domains. 10.2196/30401 2025-01-26 00:00:00 The use of artificial intelligence for automating or semi-automating biomedical literature analyses: A scoping review 2023-05-13 00:00:00 dos Santos, Álisson Oliveira | da Silva, Eduardo Sergio | Couto, Letícia Machado | Reis, Gustavo Valadares Labanca | Belo, Vinícius Silva How can artificial intelligence be applied to automate or semi-automate biomedical literature analysis, and what are the knowledge gaps in this area? This scoping review examines AI applications in automating biomedical literature analysis, covering assembly of evidence, mining biomedical texts, and quality analysis. A total of 273 studies were reviewed, with findings categorized into three groups: assembling scientific evidence (47%), mining biomedical texts (41%), and quality analysis (12%). Key gaps include methods for assessing the strength of recommendation and evidence consistency. Results indicate that while AI aids in systematic reviews and evidence summaries, its adoption by healthcare professionals remains limited. A scoping review was conducted using the Arksey and O'Malley framework and the PCC methodology. Articles were screened from databases including PubMed, Embase, and IEEE Xplore, with inclusion criteria focusing on AI tools for biomedical literature analysis. Data from 273 selected studies were categorized by application type, and descriptive analysis was performed. Studies were classified into three groups: assembly of evidence, mining biomedical texts, and quality analysis. Most focused on systematic review preparation (47%). Key tools and methods such as NLP and ML were identified as central in extracting PICO elements and summarizing evidence. The review highlights significant progress in automating biomedical literature analysis, particularly in systematic reviews. However, challenges persist, including limited tools for quality assessment and evidence synthesis. Future research should focus on improving automation for guideline preparation and expanding the use of AI by end-users. The study did not evaluate the quality of included articles and relied solely on English-language keywords, introducing potential bias. Grey literature inclusion was limited, potentially omitting relevant insights. 10.1016/j.jbi.2023.104389 2025-01-26 00:00:00 Machine learning for screening prioritization in systematic reviews: comparative performance of Abstrackr and EPPI-Reviewer 2020-03-08 00:00:00 Tsou, Amy Y. | Treadwell, Jonathan R. | Erinoff, Eileen | Schoelles, Karen How do the machine learning tools Abstrackr and EPPI-Reviewer compare in performance for citation screening prioritization in systematic reviews? This study evaluates Abstrackr and EPPI-Reviewer, two machine-learning tools, for screening prioritization in systematic reviews using nine evidence reports. The tools were tested for identifying relevant citations with potential reductions in screening burden ranging from 4% to 60%. Abstrackr outperformed in identifying articles included in final reports, while EPPI-Reviewer excelled in ordering studies for full-text review. Both tools showed variability in performance across reports. The study compared Abstrackr and EPPI-Reviewer using nine previously completed systematic review reports. Large and small datasets were tested for screening prioritization functionality. Abstrackr was trained using a random 10% sample, while EPPI-Reviewer updated iteratively. Screening was assessed incrementally to evaluate reductions in workload and prioritization accuracy. EPPI-Reviewer outperformed Abstrackr for identifying articles ordered for full-text review, reducing the screening burden by 9% to 60%. Abstrackr performed better in two of three large reports for identifying articles included in the final report. Both tools struggled with heterogeneous reports like inguinal hernia repair but performed well in tasks with clearly defined key questions. Machine learning tools improve citation screening efficiency in systematic reviews, with variability in performance based on report complexity. EPPI-Reviewer and Abstrackr demonstrated distinct strengths, supporting their complementary use in systematic review workflows. Future research should focus on enhancing these tools for varied datasets and increasing trust in automation among researchers. The study faced limitations in training set variability and dataset composition, impacting generalizability. Performance varied for smaller reports, and neither tool achieved complete sensitivity without significant manual effort. 10.1186/s13643-020-01324-7 2025-01-26 00:00:00 Comparing the Performance of CNNs and Shallow Models for Language Identification 2021-04-20 00:00:00 Ceolin, Andrea How do convolutional neural networks (CNNs) and shallow models compare in performance for language identification tasks across various datasets? This study evaluates the performance of CNNs and shallow models in language identification tasks using data from the VarDial 2021 campaign. Results show that CNNs perform comparably to shallow models like Naïve Bayes (NB) and support vector machines (SVM) in Romanian Dialect Identification (RDI) and Dravidian Language Identification (DLI) after data augmentation. However, SVMs and NB models outperformed CNNs in Uralic Language Identification (ULI), especially on imbalanced datasets. Ensemble methods combining NB and SVM were effective for rare languages in ULI. The study employed CNNs, NB, and SVM models for three shared tasks: RDI, DLI, and ULI. Models were trained on character n-grams and evaluated using macro F1 scores. Data augmentation techniques such as random word shuffling were used to improve CNN performance. Ensemble methods were also applied for specific challenges like rare language identification in ULI. CNNs achieved macro F1 scores of 0.756 in RDI with data augmentation, comparable to NB and SVM models. In ULI, ensemble methods achieved a macro F1 score of 0.905 for distinguishing rare languages. For DLI, CNNs with data augmentation achieved a micro F1 score of 0.88, slightly better than NB (0.878). Overfitting was observed in CNN models trained on small datasets. CNNs can match the performance of shallow models for language identification, but they are prone to overfitting on small datasets. Data augmentation improves CNN performance, especially for tasks involving domain shifts. Ensemble methods enhance the identification of rare languages in imbalanced datasets. Future work should focus on reducing overfitting in CNNs and improving generalization for low-resource languages. The study faced challenges with overfitting in CNNs, class imbalance, and the high computational cost of ensemble methods. Shallow models like SVM and NB showed limitations in handling high-dimensional data. 2025-01-26 00:00:00 Automatic Identification of High Impact Articles in PubMed to Support Clinical Decision Making 2017-07-26 00:00:00 Bian, Jiantao | Morid, Mohammad Amin | Jonnalagadda, Siddhartha | Luo, Gang | Del Fiol, Guilherme How can machine learning algorithms be used to identify high-impact clinical studies in PubMed to aid clinical decision-making? This study investigates machine learning algorithms for identifying high-impact clinical studies in PubMed. Using bibliometric features, social media attention, and MEDLINE metadata, the study developed a classifier optimized with a gold standard of 502 high-impact clinical studies referenced in 11 guidelines. The high-impact classifier achieved a top-20 precision of 34%, significantly outperforming PubMed's relevance sort (4%) and a state-of-the-art classifier (11%). Removing proprietary features such as citation count did not significantly degrade performance. A Naïve Bayes classifier was trained on features like citation count, journal impact factor, and Altmetric scores using a gold-standard dataset of 502 citations. Features were extracted using PubMed metadata and other sources. Performance was evaluated against PubMed's relevance sort and a prior Naïve Bayes classifier on metrics such as top-20 precision, recall, and F-measure. Statistical analyses included paired Wilcoxon tests. The high-impact classifier achieved a top-20 precision of 34%, outperforming PubMed relevance sort (4%) and the previous Naïve Bayes model (11%). Secondary metrics like mean reciprocal rank (0.78) and recall (23%) demonstrated the classifier's effectiveness. Removing citation count and Altmetric features resulted in equivalent precision (36%, p=0.085). The classifier effectively identifies high-impact articles for clinical decision support, outperforming traditional retrieval methods. Its reliance on bibliometric and social media features enhances accuracy but remains robust without proprietary features. Limitations include reliance on time-sensitive features like citation count and limited generalizability across clinical domains. Constraints include dependency on features like citation count, potential bias from using a single dataset for feature optimization, and reduced applicability for newly published studies. 10.1016/j.jbi.2017.07.015 2025-01-26 00:00:00 The Promise of Explainable AI in Digital Health for Precision Medicine: A Systematic Review 2024-03-01 00:00:00 Allen, Ben How can explainable artificial intelligence (XAI) methods be applied to enhance digital health and precision medicine while addressing challenges in transparency and accountability? This systematic review synthesizes findings from 27 studies on explainable artificial intelligence (XAI) in digital health and precision medicine. By employing a topic-modeling approach, the paper identifies key themes, including optimizing patient care, predictive modeling, and disease prediction. The review highlights XAI's role in fostering transparency, accountability, and trust, emphasizing its potential to integrate genetic, environmental, and lifestyle data for improved personalized healthcare. Challenges include ethical concerns, limited access to data in developing countries, and issues of bias in AI systems. The study conducted a systematic review using Google Scholar, focusing on peer-reviewed journal articles published in English that discussed XAI, digital health, and precision medicine. A topic-modeling approach using the latent Dirichlet allocation algorithm was employed to analyze themes across the 27 selected articles. The study adhered to the PRISMA 2020 checklist. Topic modeling identified themes including disease prediction, data-driven healthcare optimization, and the ethical challenges of XAI in precision medicine. Specific findings highlighted the role of XAI in improving diagnostic accuracy (e.g., 87% sensitivity in breast cancer diagnosis) and addressing data integration across genetic and digital health sources. XAI offers significant benefits for precision medicine by improving interpretability and trust in AI-driven healthcare systems. It fosters collaboration between human experts and machine intelligence, enabling better clinical decision-making and patient outcomes. However, ethical issues like data privacy and transparency need further attention. Future work should focus on developing open-source tools and policies to enhance data accessibility and minimize bias. The study was limited to English-language articles, potentially introducing selection bias. It also excluded articles behind paywalls, limiting comprehensiveness. 10.3390/jpm14030277 2025-01-26 00:00:00 Survey of Explainable AI Techniques in Healthcare 2023-01-05 00:00:00 Chaddad, Ahmad | Peng, Jihao | Xu, Jian | Bouridane, Ahmed What are the current explainable AI (XAI) techniques applied in healthcare, and how can these methods address challenges in interpretability and trust in medical applications? This survey reviews recent XAI techniques applied in healthcare and medical imaging, focusing on methods to enhance interpretability and trust in AI models. The study categorizes XAI methods into intrinsic and post hoc models and discusses their application in tasks such as medical imaging and text analysis. Challenges in ethical considerations, data privacy, and model bias are highlighted, alongside future directions for improving transparency and collaboration between academic and clinical research. Recommendations include designing XAI methods tailored for specific clinical applications to address the unique challenges of medical datasets. The paper uses a systematic approach to review and categorize XAI methods in healthcare. It focuses on widely used techniques such as Class Activation Mapping (CAM), Grad-CAM, SHAP, LIME, and prototype-based networks. It provides an overview of their applications in radiomics and medical imaging for disease diagnosis and risk assessment. XAI techniques such as CAM and Grad-CAM have proven effective for visualizing important areas in medical images. SHAP and LIME provide feature importance explanations for electronic health records. Prototype-based methods enhance interpretability by mapping input features to learned prototypes for classification tasks. However, challenges like computational costs and handling bias persist. XAI enhances trust and interpretability in healthcare AI applications, but challenges remain in addressing model biases and ensuring ethical usage. Future work should focus on developing robust, efficient, and ethical XAI methods tailored to medical applications. Collaborative efforts between AI developers and clinicians are essential for creating tools that meet real-world needs. Limited focus on underrepresented healthcare applications and reliance on English-language literature, which may introduce selection bias. Computational overhead for some XAI methods is also a constraint. 10.3390/s23020634 2025-01-26 00:00:00 Avoiding Communication in Logistic Regression 2020-11-16 00:00:00 Devarakonda, Aditya | Demmel, James How can communication overhead in stochastic gradient descent (SGD) for logistic regression be reduced without altering convergence behavior or accuracy? This paper introduces a communication-avoiding technique for stochastic gradient descent (CA-SGD) to optimize logistic regression. By reorganizing computations to communicate every s iterations instead of every iteration, CA-SGD achieves significant reductions in latency costs while preserving convergence and accuracy. Experimental results demonstrate up to 4.97× speedup over traditional SGD in distributed systems. Theoretical bounds on computation, bandwidth, and latency costs are derived and validated through performance experiments on high-performance clusters. CA-SGD modifies SGD by grouping computations into blocks of s iterations, reducing inter-process communication. Numerical stability is maintained by unrolling recurrence relations and leveraging matrix-vector multiplication properties. Experiments were conducted on logistic regression tasks using sparse datasets distributed across processors, with performance compared to traditional SGD. CA-SGD achieved up to 4.97× speedup over SGD, with latency reduction by a tunable factor of s. Numerical experiments validated stability, showing negligible floating-point errors even for large s values. Scalability tests demonstrated improved performance on high-latency distributed systems, with CA-SGD extending to 4× as many cores. CA-SGD effectively reduces latency in logistic regression tasks, achieving significant speedups while maintaining convergence behavior. The method is particularly beneficial in distributed environments where latency dominates computational costs. Future research will explore applying CA-SGD to neural networks and other optimization tasks. The approach may face challenges in bandwidth-limited environments, as additional computation and communication costs grow with batch size and s. Applications to dense datasets or low-latency environments may yield modest speedups. 2025-01-26 00:00:00 Using the Contextual Language Model BERT for Multi-Criteria Classification of Scientific Articles 2020-10-13 00:00:00 Ambalavanan, Ashwin Karthik | Devarakonda, Murthy V. How can BERT-based models improve multi-criteria classification of scientific articles, and what are the optimal strategies for combining different criteria in ensemble approaches? This study investigates the use of BERT-based models, specifically SciBERT, for classifying scientific articles across multiple criteria. By experimenting with ensemble architectures and a single integrated model, the study aims to optimize the classification of 49,028 MEDLINE abstracts annotated on four criteria. The cascade ensemble model outperformed other architectures in precision and F-measure but had lower recall. The single integrated model achieved the highest recall, making it suitable for systematic reviews. The dataset was sourced from the Clinical Hedges project, comprising 49,028 MEDLINE articles annotated on criteria such as format, relevance to human healthcare, purpose, and scientific rigor. Experiments compared a single integrated model (ITL) to ensemble methods, including cascade, Boolean, and feed-forward network (FFN) ensembles. The study used SciBERT with various training configurations, sampling ratios, and sequence lengths. The cascade ensemble achieved the highest precision (0.663) and F-measure (0.753), while the ITL achieved the highest recall (0.965). The ensemble-Boolean and ensemble-FFN methods showed balanced trade-offs between precision and recall but lagged behind in overall performance. Increasing sampling ratios improved model performance but required substantial computational resources. SciBERT-based models effectively classify scientific articles across multiple criteria. Cascade ensembles excel in precision and F-measure, suitable for low-noise applications like search. Single integrated models are better for high-recall tasks like systematic reviews. Future work should focus on combining the strengths of both architectures and optimizing training ratios for imbalanced datasets. Limitations include reliance on a single dataset, challenges in modeling the scientific rigor criterion, and computational constraints for large training samples. Results may not generalize to other datasets or BERT variants. 10.1016/j.jbi.2020.103578 2025-01-26 00:00:00 Methodological Challenges in Randomized Controlled Trials of mHealth Interventions: Cross-Sectional Survey Study and Consensus-Based Recommendations 2024-01-15 00:00:00 Lopez-Alcalde, Jesus | Wieland, L. Susan | Yan, Yuqian | Barth, Jürgen | Khami, Mohammad Reza | Shivalli, Siddharudha | Lokker, Cynthia | Rai, Harleen Kaur | Macharia, Paul | Yun, Sergi | Lang, Elvira | Naggirinya, Agnes Bwanika | Campos-Asensio, Concepción | Ahmadian, Leila | Witt, Claudia M. What are the unique methodological challenges in randomized controlled trials (RCTs) of mobile health (mHealth) interventions, and how can consensus-based recommendations address them? This study identifies methodological challenges in RCTs of mHealth interventions and develops 17 consensus-based recommendations to address them. The research involved a two-phase approach: a web-based survey of 1535 authors of mHealth RCTs and an online workshop with selected respondents. Key challenges included managing low adherence, defining adherence, measuring adherence, and analyzing passive data. Recommendations emphasized strategies to improve adherence measurement, data privacy, and intervention integrity, as well as guidance on using app updates and tailoring control conditions. The study utilized a mixed-methods design comprising a web-based survey and an online workshop. The survey targeted authors of mHealth RCTs, identifying 21 methodological challenges. Participants rated these on a 5-point scale, and responses were analyzed using descriptive statistics. A thematic analysis of the workshop discussions informed the development of recommendations, which were refined through email feedback. Survey responses (n=80) revealed that adherence-related challenges were the most significant, including managing low adherence (56% of respondents) and defining adherence (49%). Workshop participants (n=11) developed 17 recommendations, such as using diverse adherence measurement methods, integrating privacy policies, and considering the impact of app updates. The study highlighted variability in challenges across experience levels and geographical regions. Addressing methodological challenges in mHealth RCTs is crucial for enhancing intervention reliability and validity. Recommendations provide a framework for measuring adherence, ensuring data privacy, and managing app updates, enabling more robust evaluations of mHealth interventions. Future research should refine adherence metrics and explore their application in diverse contexts. The study had a low survey response rate (5.21%) and limited representation of researchers with experience in both mHealth and non-mHealth RCTs. Variability among mHealth intervention types and contexts may affect the generalizability of findings. 10.2196/53187 2025-01-26 00:00:00 Boosting Efficiency in a Clinical Literature Surveillance System with LightGBM 2024-09-23 00:00:00 Lokker, Cynthia | Abdelkader, Wael | Bagheri, Elham | Parrish, Rick | Cotoi, Chris | Navarro, Tamara | Germini, Federico | Linkins, Lori-Ann | Haynes, R. Brian | Chu, Lingyang | Afzal, Muhammad | Iorio, Alfonso How can the use of the LightGBM machine learning algorithm improve the efficiency and accuracy of clinical literature surveillance systems? This research explores using LightGBM, a gradient-boosting machine learning model, to classify clinical articles based on methodological rigor and clinical relevance. Models trained on a dataset of 97,805 PubMed articles (2012–2018) achieved 99% sensitivity and improved specificity, reducing the number of articles required to be reviewed. LightGBM improved efficiency, reducing the workload by 45% in a prospective evaluation. The model demonstrated scalability and real-world applicability, including application to COVID-19 literature. The study employed Microsoft AutoML to train LightGBM models on PubMed data labeled for rigor and relevance. The dataset was manipulated through oversampling and undersampling to address class imbalance. The models were tested on 20% of the dataset, validated on 30,424 articles from 2020, and prospectively evaluated on 5,253 articles from 2021. Evaluation metrics included sensitivity, specificity, F-score, and number-needed-to-read (NNR). Models achieved 99% sensitivity and specificity values of 57% in retrospective and 53% in prospective evaluations. The NNR was reduced to 3.68 in the prospective evaluation, representing a 45% efficiency gain. Model performance was consistent across imbalanced and resampled datasets. LightGBM successfully optimized clinical literature surveillance, balancing high sensitivity and moderate specificity. While improving efficiency, it maintained the integrity of evidence selection, reducing human effort. Future research includes adapting models for diverse datasets and broader applications in evidence-based medicine. Reliance on PubMed metadata limited generalizability. Oversampling and undersampling introduced biases that could affect calibration. Models were trained on abstracts and titles without full-text access, potentially missing critical appraisal details. 10.1371/journal.pdig.0000299 2025-02-04 00:00:00 Attitudes and Perceptions of Medical Researchers Towards the Use of Artificial Intelligence Chatbots in the Scientific Process: An International Cross-Sectional Survey 2024-11-15 00:00:00 Ng, Jeremy Y | Maduranayagam, Sharleen G | Suthakar, Nirekah | Li, Amy | Lokker, Cynthia | Iorio, Alfonso | Haynes, R Brian | Moher, David What are medical researchers' attitudes, familiarity, perceived benefits, and limitations regarding the use of AI chatbots in the scientific research process? AI chatbots offer both opportunities and challenges in scientific research, yet researchers' perceptions remain unclear. This study conducted an international cross-sectional survey from July 9 to August 11, 2023, targeting 61,560 corresponding authors from PubMed-indexed articles. A total of 2,452 responses were collected (4.0% response rate), with 2,165 completing the survey. Findings revealed that 60.5% of respondents were familiar with AI chatbots, and 44.5% had previously used them in research. Institutional support was lacking, with only 11.4% receiving AI-related training and 9.9% reporting institutional policies. Although AI chatbots were valued for reducing administrative workload (66.9%), 77.2% of respondents indicated a lack of understanding of their decision-making processes. The study highlights growing interest in AI chatbots but underscores the need for formal training and clearer guidelines for their implementation in scientific research. A cross-sectional survey was conducted from July 9 to August 11, 2023, targeting corresponding authors from PubMed-indexed articles published between March 1 and April 7, 2023. The survey contained 29 questions covering demographic details, AI chatbot familiarity, benefits, challenges, and institutional policies. Data were analyzed using descriptive statistics, thematic coding, and frequency distributions. 2,165 respondents completed the survey (94.5% of eligible participants). 60.5% were familiar with AI chatbots; 44.5% had used them in research. 11.4% received institutional AI-related training; 9.9% reported institutional AI policies. 66.9% found AI chatbots helpful in reducing administrative workload. 69.7% expressed interest in further AI chatbot training. 77.2% reported a lack of understanding of AI chatbot decision-making. ChatGPT was the most commonly used AI chatbot (65.6%). 37% of respondents were likely to use AI chatbots in future research. AI chatbots are increasingly used by researchers, with a majority recognizing their value in administrative tasks, manuscript writing, and research assistance. However, a lack of transparency in decision-making and insufficient institutional support limit their effectiveness. The study emphasizes the need for structured training, institutional guidelines, and ethical considerations to ensure AI chatbots enhance, rather than undermine, scientific integrity. The study had a low response rate (4.0%), which may limit generalizability. Non-English-speaking researchers were largely excluded, and institutional policies on AI chatbots remain unclear. The survey captures a single point in time, preventing long-term trend analysis. 10.1016/S2589-7500(24)00202-4 2025-02-08 00:00:00 An Open Source Machine Learning Framework for Efficient and Transparent Systematic Reviews 2021-02-01 00:00:00 van de Schoot, Rens | de Bruin, Jonathan | Schram, Raoul | Zahedi, Parisa | de Boer, Jan | Weijdema, Felix | Kramer, Bianca | Huijts, Martijn | Hoogerwerf, Maarten | Ferdinands, Gerbrich | Harkema, Albert | Willemsen, Joukje | Ma, Yongchao | Fang, Qixiang | Hindriks, Sybren | Tummers, Lars | Oberski, Daniel L. How can an open-source machine learning framework enhance efficiency and transparency in conducting systematic reviews? This study presents ASReview, an open-source machine-learning pipeline designed to accelerate systematic reviews by employing active learning techniques. Given the exponential growth of scientific publications, manual systematic reviewing is becoming increasingly impractical. ASReview aims to improve efficiency by prioritizing relevant studies for review while ensuring transparency in the process. Through simulation studies, the paper demonstrates that active learning significantly reduces the number of articles requiring manual screening without compromising review quality. The paper also describes the tool’s functionality, user experience testing, and the potential applications of ASReview across different domains. The study employed an open-source software development approach, integrating active learning algorithms to optimize systematic reviewing. ASReview was tested using simulation studies across multiple datasets from various domains, including software engineering, biomedicine, and public health. User experience (UX) testing was conducted via interviews and remote usability studies, refining the software based on expert feedback. Performance metrics, such as Work Saved over Sampling (WSS) and Recall Rate after screening a percentage of abstracts (RRF10%), were used to assess the effectiveness of the tool. The simulations showed that ASReview achieved an average work saved over sampling (WSS@95%) of 83%, reducing the number of manually screened abstracts by up to 92% while maintaining high recall rates. User tests indicated that ASReview improved the efficiency and accuracy of systematic reviews, with participants rating the tool an average of 7.9/10 in usability tests. ASReview significantly reduces the workload involved in systematic reviews while maintaining transparency and reproducibility. The software allows researchers to screen large datasets more efficiently, making systematic reviewing more scalable. The study highlights the importance of integrating machine learning with human expertise in the research process. Future research should focus on expanding the tool’s applicability to different domains and integrating automated benchmarking systems. The system relies on the quality of the initial search query and user-defined prior knowledge for training. The effectiveness of active learning depends on the availability of relevant initial labels. The study also notes the challenge of estimating error rates without additional labeling efforts. https://doi.org/10.1038/s42256-020-00287-7 2025-02-08 00:00:00 Methodological Challenges in Randomized Controlled Trials of mHealth Interventions: Cross-Sectional Survey Study and Consensus-Based Recommendations 2024 Lopez-Alcalde, Jesus | Wieland, L Susan | Yan, Yuqian | Barth, Jürgen | Khami, Mohammad Reza | Shivalli, Siddharudha | Lokker, Cynthia | Rai, Harleen Kaur | Macharia, Paul | Yun, Sergi | Lang, Elvira | Naggirinya, Agnes Bwanika | Campos-Asensio, Concepción | Ahmadian, Leila | Witt, Claudia M. What are the specific methodological challenges in randomized controlled trials (RCTs) of mHealth interventions, and how can consensus-based recommendations address these challenges? This study identifies and addresses specific methodological challenges in RCTs of mHealth interventions. A web-based survey was conducted among authors of mHealth RCTs to evaluate 21 methodological aspects. Additionally, a subset of respondents participated in an online workshop to develop consensus-based recommendations. The study found that issues related to mHealth intervention integrity, particularly defining, measuring, and managing adherence, were the most frequently reported challenges. Other difficulties included analyzing passive data and verifying participant identity. Based on these findings, 17 consensus-based recommendations were formulated to enhance the reliability of mHealth RCTs. The study followed a two-phase participatory research approach. In Phase 1, a web-based survey was distributed to 1535 authors of mHealth RCTs, with 80 completing the survey. The survey assessed 21 methodological challenges on a 5-point scale. In Phase 2, 11 researchers participated in an online workshop where consensus-based recommendations were developed. The recommendations were then refined via email exchanges and thematic analysis. The survey revealed that 92% of respondents identified at least one methodological challenge as more difficult in mHealth RCTs compared to non-mHealth RCTs. The most frequently cited challenges were related to intervention integrity, including low adherence rates (56%), defining adherence (49%), and measuring adherence (42%). Additional challenges included analyzing passive data (41%) and verifying participant identity (41%). The workshop resulted in 17 consensus-based recommendations focusing on adherence measurement, intervention integrity, and handling missing data. RCTs of mHealth interventions face unique challenges, particularly in ensuring intervention integrity. Addressing these challenges through structured recommendations can improve the validity and reliability of mHealth RCTs. The study emphasizes the need for methodological guidance on defining and measuring adherence, improving data collection strategies, and adapting RCT designs to the dynamic nature of mHealth interventions. The study had a low survey response rate (5.21%), which may limit the generalizability of findings. Additionally, most survey respondents had experience exclusively with mHealth RCTs, limiting direct comparisons with non-mHealth RCTs. The study did not account for variations between different types of mHealth interventions. https://doi.org/10.2196/53187",
    "metadata": {
      "source": "C:\\Users\\ASUS\\OneDrive\\Documents\\GitHub\\Anki_LLM_withQA\\src\\projects\\File_type_test\\raw_pdfs\\LitRev.xlsx",
      "chunk_id": "LitRev_para0"
    }
  }
]