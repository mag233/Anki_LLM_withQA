# rag_tab.py
import os
import json
import streamlit as st
from preprocess import process_documents
from embed import create_or_update_embeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

def render_rag_tab(PROJECTS_DIR):
    st.header("RAG: é¡¹ç›®ä¸æ–‡ä»¶ç®¡ç†")

    # â€”â€” é¡¹ç›®é€‰æ‹©ä¸æ–°å»º â€”â€” 
    projects = [d for d in os.listdir(PROJECTS_DIR) if os.path.isdir(os.path.join(PROJECTS_DIR, d))]
    idx = 1 if projects else 0
    selected = st.selectbox("é€‰æ‹©é¡¹ç›®ï¼š", ["æ–°å»ºé¡¹ç›®"] + projects, index=idx, key="rag_project")

    if selected == "æ–°å»ºé¡¹ç›®":
        nm = st.text_input("è¯·è¾“å…¥æ–°é¡¹ç›®åç§°ï¼š")
        if st.button("åˆ›å»ºé¡¹ç›®") and nm:
            p = os.path.join(PROJECTS_DIR, nm)
            os.makedirs(os.path.join(p, "raw_pdfs"), exist_ok=True)
            os.makedirs(os.path.join(p, "processed", "chunks"), exist_ok=True)
            os.makedirs(os.path.join(p, "vectorstore", "chroma_db"), exist_ok=True)
            st.success(f"é¡¹ç›® '{nm}' åˆ›å»ºæˆåŠŸï¼")
            st.experimental_rerun()

    # ç¡®ä¿æœ‰å·²é€‰é¡¹ç›®
    if selected and selected != "æ–°å»ºé¡¹ç›®":
        # è·¯å¾„
        base        = os.path.join(PROJECTS_DIR, selected)
        raw_dir     = os.path.join(base, "raw_pdfs")
        proc_dir    = os.path.join(base, "processed")
        chunks_dir  = os.path.join(proc_dir, "chunks")
        db_dir      = os.path.join(base, "vectorstore", "chroma_db")
        manifest_fp = os.path.join(proc_dir, "manifest.json")

        # ç¡®ä¿ç›®å½•
        for d in [raw_dir, chunks_dir, db_dir]:
            os.makedirs(d, exist_ok=True)

        # â€”â€” æ­¥éª¤1ï¼šä¸Šä¼ æ–‡ä»¶ â€”â€” 
        st.markdown("### æ­¥éª¤1ï¼šä¸Šä¼ æ–‡ä»¶")
        ups = st.file_uploader(
            "ä¸Šä¼  PDF/MD/DOCX/XLSX/HTML",
            type=["pdf","md","docx","xlsx","html"], accept_multiple_files=True
        )
        new_files = []
        if ups:
            exist = set(os.listdir(raw_dir))
            for f in ups:
                if f.name in exist:
                    st.warning(f"'{f.name}' å·²å­˜åœ¨ã€‚")
                else:
                    with open(os.path.join(raw_dir, f.name), "wb") as fw:
                        fw.write(f.getbuffer())
                    new_files.append(f.name)
            if new_files:
                st.success("å·²ä¸Šä¼ : " + ", ".join(new_files))
            else:
                st.info("æ— æ–°æ–‡ä»¶ã€‚")
        else:
            st.caption("è¯·ä¸Šä¼ æ–‡ä»¶ã€‚")

        # â€”â€” å„ç±»æ•°æ®å±•ç¤ºï¼ˆä»ªè¡¨æ¿ï¼‰ â€”â€” 
        st.divider()
        st.markdown("#### å½“å‰åˆ†å— & åµŒå…¥ çŠ¶æ€")
        embed_model = os.getenv("EMBED_MODEL", "text-embedding-3-large")
        st.info(f"åµŒå…¥æ¨¡å‹: **{embed_model}**")

        # chunks æ•°é‡
        chunk_count = sum(
            len(json.load(open(os.path.join(chunks_dir, fn), "r", encoding="utf-8")))
            for fn in os.listdir(chunks_dir) if fn.endswith("_chunks.json")
        )
        st.info(f"å½“å‰åˆ†å—æ€»æ•°: **{chunk_count}**")

        # embeddings æ•°é‡
        try:
            db = Chroma(
                persist_directory=db_dir,
                embedding_function=OpenAIEmbeddings(model=embed_model),
                collection_name="literature_chunks"
            )
            embed_count = len(db._collection.get()["ids"])
        except:
            embed_count = 0
        st.info(f"å½“å‰ embeddings æ•°é‡: **{embed_count}**")

        # è¿›åº¦æ¡
        prog = (embed_count / chunk_count) if chunk_count else 0.0
        st.progress(prog, text="åµŒå…¥è¿›åº¦ï¼ˆå·²å…¥åº“/åˆ†å—ï¼‰")

        # manifest è¡¨æ ¼
        st.markdown("#### ğŸ“„ æ–‡ä»¶å¤„ç†çŠ¶æ€ (manifest)")
        if os.path.exists(manifest_fp):
            mf = json.load(open(manifest_fp, "r", encoding="utf-8"))
            rows = []
            for fn, m in mf.items():
                stem = os.path.splitext(fn)[0]
                rows.append({
                    "æ–‡ä»¶": fn,
                    "åˆ†å—æ•°": m.get("n_chunks", "-"),
                    "åˆ†å—æ–¹å¼": m.get("chunk_method", "-"),  # æ–°å¢
                    "æœ€åå¤„ç†": m.get("last_processed", "-")
                })
            st.dataframe(
                rows,
                hide_index=True,
                use_container_width=True,
                height=350  # å›ºå®šé«˜åº¦ï¼Œæ”¯æŒæ»šåŠ¨
            )
        else:
            st.info("æš‚æ—  manifest.jsonï¼Œå°šæœªé¢„å¤„ç†ã€‚")

        # â€”â€” æ­¥éª¤2ï¼šé¢„å¤„ç†æ–‡ä»¶ï¼ˆåˆ†å—ï¼‰ â€”â€” 
        st.divider()
        st.markdown("### æ­¥éª¤2ï¼šé¢„å¤„ç†æ–‡ä»¶ï¼ˆåˆ†å—ï¼‰")

        method = st.selectbox("åˆ†å—æ–¹å¼ï¼š", ["æŒ‰å¥å­","æŒ‰æ®µè½","æŒ‰é¡µ","å›ºå®šé•¿åº¦"], index=0)
        if method == "å›ºå®šé•¿åº¦":
            size = st.number_input("åˆ†å—é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰", min_value=50, max_value=2000, value=400, step=50)
        else:
            size = None

        force = st.checkbox(
            "å¼ºåˆ¶å…¨éƒ¨é‡æ–°é¢„å¤„ç†ï¼ˆå¿½ç•¥ hashï¼Œé€‚ç”¨äºå‚æ•°å˜æ›´æˆ–ä¿®å¤ï¼‰",
            value=False
        )

        if st.button("å¼€å§‹é¢„å¤„ç†"):
            try:
                process_documents(
                    raw_dir,
                    proc_dir,
                    extract_tables=True,
                    extract_images=True,
                    extract_meta=True,
                    chunking_method=method,
                    chunk_size=size or 400,
                    chunk_overlap=50,
                    force_reprocess=force
                )
                st.success("é¢„å¤„ç†å®Œæˆï¼")
                # é¢„å¤„ç†åå¼ºåˆ¶åˆ·æ–° manifest å’Œè®¡æ•°
                if os.path.exists(manifest_fp):
                    mf = json.load(open(manifest_fp, "r", encoding="utf-8"))
                else:
                    mf = {}
                chunk_count = sum(
                    len(json.load(open(os.path.join(chunks_dir, fn), "r", encoding="utf-8")))
                    for fn in os.listdir(chunks_dir) if fn.endswith("_chunks.json")
                )
                try:
                    db = Chroma(
                        persist_directory=db_dir,
                        embedding_function=OpenAIEmbeddings(model=embed_model),
                        collection_name="literature_chunks"
                    )
                    embed_count = len(db._collection.get()["ids"])
                except:
                    embed_count = 0
                st.info(f"å½“å‰åˆ†å—æ€»æ•°: **{chunk_count}**")
                st.info(f"å½“å‰ embeddings æ•°é‡: **{embed_count}**")
                prog = (embed_count / chunk_count) if chunk_count else 0.0
                st.progress(prog, text="åµŒå…¥è¿›åº¦ï¼ˆå·²å…¥åº“/åˆ†å—ï¼‰")
                # é‡æ–°æ˜¾ç¤º manifest è¡¨æ ¼ï¼ˆé¢„å¤„ç†ååˆ·æ–°ï¼‰
                rows = []
                for fn, m in mf.items():
                    stem = os.path.splitext(fn)[0]
                    rows.append({
                        "æ–‡ä»¶": fn,
                        "åˆ†å—æ•°": m.get("n_chunks", "-"),
                        "åˆ†å—æ–¹å¼": m.get("chunk_method", "-"),  # æ–°å¢
                        "æœ€åå¤„ç†": m.get("last_processed", "-")
                    })
                st.dataframe(
                    rows,
                    hide_index=True,
                    use_container_width=True,
                    height=350  # å›ºå®šé«˜åº¦ï¼Œæ”¯æŒæ»šåŠ¨
                )
            except Exception as e:
                st.error(f"é¢„å¤„ç†å¤±è´¥: {e}")

        # â€”â€” æ­¥éª¤3ï¼šç”Ÿæˆ/æ›´æ–°åµŒå…¥ â€”â€” 
        st.divider()
        st.markdown("### æ­¥éª¤3ï¼šç”Ÿæˆ/æ›´æ–°åµŒå…¥")
        mode = st.radio("åµŒå…¥æ–¹å¼ï¼š", ["å…¨éƒ¨é‡æ–°åµŒå…¥","ä»…æ–°å¢åˆ†å—"], index=0)
        stems = {os.path.splitext(f)[0] for f in new_files}
        if st.button("ç”ŸæˆåµŒå…¥"):
            duplicate_ids = []
            try:
                only = stems if mode.endswith("æ–°å¢åˆ†å—") else None
                try:
                    create_or_update_embeddings(chunks_dir, db_dir, only_files=only)
                    st.success("åµŒå…¥å®Œæˆï¼")
                except ValueError as ve:
                    msg = str(ve)
                    if "Expected IDs to be unique" in msg:
                        # æå–é‡å¤ID
                        import re
                        dup_match = re.findall(r"found duplicates of: ([^ ]+)", msg)
                        duplicate_ids.extend(dup_match)
                        st.warning(f"æ£€æµ‹åˆ°é‡å¤IDï¼Œå·²è·³è¿‡: {', '.join(duplicate_ids)}")
                        print(f"[Embed] Duplicate IDs skipped: {duplicate_ids}")
                        st.info("éƒ¨åˆ†åˆ†å—å› IDé‡å¤æœªè¢«åµŒå…¥ï¼Œå…¶ä½™å·²å®Œæˆã€‚")
                    else:
                        st.error(f"åµŒå…¥å¤±è´¥: {ve}")
                except Exception as e:
                    st.error(f"åµŒå…¥å¤±è´¥: {e}")
            except Exception as e:
                st.error(f"åµŒå…¥å¤±è´¥: {e}")

        # â€”â€” Manifest å¥åº·æ£€æŸ¥ â€”â€” 
        st.divider()
        st.markdown("#### ğŸ“Š Manifest å¥åº·æ£€æŸ¥")
        if os.path.exists(manifest_fp):
            mf = json.load(open(manifest_fp, "r", encoding="utf-8"))
            missing = [k for k, v in mf.items() if v.get("n_chunks", 0) == 0]
            if missing:
                st.warning(f"æœªç”Ÿæˆåˆ†å—çš„æ–‡ä»¶: {', '.join(missing)}")
            else:
                st.success("æ‰€æœ‰æ–‡ä»¶å‡å·²åˆ†å—ã€‚")
